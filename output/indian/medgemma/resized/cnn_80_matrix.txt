Classification Report:
              precision    recall  f1-score   support

           0      0.882     0.441     0.588        34
           1      0.100     0.200     0.133         5
           2      0.438     0.438     0.438        32
           3      0.444     0.632     0.522        19
           4      0.412     0.538     0.467        13

    accuracy                          0.476       103
   macro avg      0.455     0.450     0.429       103
weighted avg      0.566     0.476     0.492       103

Confusion Matrix:
[[15  6 13  0  0]
 [ 1  1  3  0  0]
 [ 0  2 14 11  5]
 [ 1  1  0 12  5]
 [ 0  0  2  4  7]]
Binary Referral Classification Report (based on grade):
              precision    recall  f1-score   support

 No Referral       0.93      0.77      0.85        71
    Referral       0.64      0.88      0.74        32

    accuracy                           0.81       103
   macro avg       0.78      0.82      0.79       103
weighted avg       0.84      0.81      0.81       103

Confusion Matrix:
[[55 16]
 [ 4 28]]
Binary Referral Classification Report (based on `referral` field):
              precision    recall  f1-score   support

 No Referral       0.95      0.58      0.72        71
    Referral       0.50      0.94      0.65        32

    accuracy                           0.69       103
   macro avg       0.73      0.76      0.69       103
weighted avg       0.81      0.69      0.70       103

Confusion Matrix:
[[41 30]
 [ 2 30]]


 Metric	Zero-Shot LLM	+ CNN Context (80%)	Change
Accuracy	29.1%	47.6%	↑ +18.5%
Macro F1	27.5%	42.9%	↑ +15.4%
Weighted F1	31.9%	49.2%	↑ +17.3%
Class 0 Recall	17.6%	44.1%	↑
Class 3 F1	43.5%	52.2%	↑

Key takeaway:

Class 0 (no DR) improved significantly in recall, and there were broad gains in 
almost every class. Class 1 is still weak, likely due to low support (n=5), 
but overall, prompting with CNN context led to stronger alignment with ground truth across more categories.